"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[254],{3905:function(t,e,n){n.d(e,{Zo:function(){return u},kt:function(){return g}});var a=n(7294);function r(t,e,n){return e in t?Object.defineProperty(t,e,{value:n,enumerable:!0,configurable:!0,writable:!0}):t[e]=n,t}function l(t,e){var n=Object.keys(t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(t);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),n.push.apply(n,a)}return n}function i(t){for(var e=1;e<arguments.length;e++){var n=null!=arguments[e]?arguments[e]:{};e%2?l(Object(n),!0).forEach((function(e){r(t,e,n[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(n,e))}))}return t}function p(t,e){if(null==t)return{};var n,a,r=function(t,e){if(null==t)return{};var n,a,r={},l=Object.keys(t);for(a=0;a<l.length;a++)n=l[a],e.indexOf(n)>=0||(r[n]=t[n]);return r}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(a=0;a<l.length;a++)n=l[a],e.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(t,n)&&(r[n]=t[n])}return r}var o=a.createContext({}),d=function(t){var e=a.useContext(o),n=e;return t&&(n="function"==typeof t?t(e):i(i({},e),t)),n},u=function(t){var e=d(t.components);return a.createElement(o.Provider,{value:e},t.children)},m={inlineCode:"code",wrapper:function(t){var e=t.children;return a.createElement(a.Fragment,{},e)}},k=a.forwardRef((function(t,e){var n=t.components,r=t.mdxType,l=t.originalType,o=t.parentName,u=p(t,["components","mdxType","originalType","parentName"]),k=d(n),g=r,s=k["".concat(o,".").concat(g)]||k[g]||m[g]||l;return n?a.createElement(s,i(i({ref:e},u),{},{components:n})):a.createElement(s,i({ref:e},u))}));function g(t,e){var n=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var l=n.length,i=new Array(l);i[0]=k;var p={};for(var o in e)hasOwnProperty.call(e,o)&&(p[o]=e[o]);p.originalType=t,p.mdxType="string"==typeof t?t:r,i[1]=p;for(var d=2;d<l;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}k.displayName="MDXCreateElement"},9414:function(t,e,n){n.r(e),n.d(e,{assets:function(){return u},contentTitle:function(){return o},default:function(){return g},frontMatter:function(){return p},metadata:function(){return d},toc:function(){return m}});var a=n(7462),r=n(3366),l=(n(7294),n(3905)),i=["components"],p={id:"lectures",title:"Course Schedule"},o=void 0,d={unversionedId:"lectures",id:"lectures",title:"Course Schedule",description:"| wk | Lecture | Links | Reading material |",source:"@site/../docs/lectures.md",sourceDirName:".",slug:"/lectures",permalink:"/ml-class/docs/lectures",draft:!1,tags:[],version:"current",lastUpdatedAt:1660047351,formattedLastUpdatedAt:"Aug 9, 2022",frontMatter:{id:"lectures",title:"Course Schedule"}},u={},m=[],k={toc:m};function g(t){var e=t.components,n=(0,r.Z)(t,i);return(0,l.kt)("wrapper",(0,a.Z)({},k,n,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"wk"),(0,l.kt)("th",{parentName:"tr",align:null},"Lecture"),(0,l.kt)("th",{parentName:"tr",align:null},"Links"),(0,l.kt)("th",{parentName:"tr",align:null},"Reading material"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/1"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-0 Release"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://campuswire.com/c/G7204E992/feed/2"},"Campuswire")),(0,l.kt)("td",{parentName:"tr",align:null},"Fundamentals of Linear Algebra and Deep Learning")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/3"),(0,l.kt)("td",{parentName:"tr",align:null},"Part 1: Course overview ",(0,l.kt)("br",null),"Part 2: Supervised learning for control"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("br",null),"1","."," ",(0,l.kt)("a",{parentName:"td",href:"http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf"},"A framework for behavioural cloning, Bain and Sommut, 1999."),(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"http://proceedings.mlr.press/v15/ross11a/ross11a.pdf"},"A reduction of imitation learning and structured prediction to no-regret online learning, Ross et al., 2011."))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/10"),(0,l.kt)("td",{parentName:"tr",align:null},"Part 1: Introduction to RL",(0,l.kt)("br",null),"Part 2: Value functions"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Part 1: Chapter 1 of ",(0,l.kt)("a",{parentName:"td",href:"http://incompleteideas.net/book/RLbook2020.pdf"},"RL book"),".",(0,l.kt)("br",null),"Part 2: Chapter 3 of ",(0,l.kt)("a",{parentName:"td",href:"http://incompleteideas.net/book/RLbook2020.pdf"},"RL book"),".")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/10"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-0 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/10"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-1 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Imitation via Supervision.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/17"),(0,l.kt)("td",{parentName:"tr",align:null},"Deep Q Learning"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://daiwk.github.io/assets/dqn.pdf"},"Human-level control through deep reinforcement learning, Mnih et al., 2015"),".",(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1710.02298.pdf"},"Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al., 2017."),"\xa0",(0,l.kt)("br",null)," 3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark"},"Agent 57, Deepmind blog post"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/24"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-1 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/24"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-2 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Deep Q Learning++.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/24"),(0,l.kt)("td",{parentName:"tr",align:null},"Policy gradients"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf"},"REINFORCE, Williams, 1992."),(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf"},"A Natural Policy Gradient, Kakade, 2002"),".",(0,l.kt)("br",null),"3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1707.06347.pdf"},"Proximal Policy Optimization Algorithms, Schulman et al. 2017."))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/1"),(0,l.kt)("td",{parentName:"tr",align:null},"Part 1: Actor-critic methods",(0,l.kt)("br",null),"Part 2: Distributed RL"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"http://proceedings.mlr.press/v32/silver14.pdf"},"Deterministic Policy Gradient Algorithms, Silver et al., 2014."),(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1602.01783.pdf"},"Asynchronous Methods for Deep Reinforcement Learning, Mnih et al. 2016."),(0,l.kt)("br",null),"3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1507.04296.pdf"},"Massively Parallel Methods for Deep Reinforcement Learning, Nair et al. 2015."))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/8"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-2 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/8"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-3 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"RL with Policy Gradients.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/8"),(0,l.kt)("td",{parentName:"tr",align:null},"Exploration in RL"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html"},"Exploration strategies in deep RL, blog by Lilian Weng, 2020")," ",(0,l.kt)("br",null)," 2","."," ",(0,l.kt)("a",{parentName:"td",href:"http://www.pyoudeyer.com/ims.pdf"},"Intrinsic Motivation Systems for Autonomous Mental Development, Oudeyer et al. 2007.")," ",(0,l.kt)("br",null)," 3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1705.05363.pdf"},"Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al. 2017."))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/9"),(0,l.kt)("td",{parentName:"tr",align:null},"Project Proposals Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/15"),(0,l.kt)("td",{parentName:"tr",align:null},"Generalization in RL"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://openai.com/blog/quantifying-generalization-in-reinforcement-learning"},"Quantifying Generalization in RL, blog by OpenAI 2018"),(0,l.kt)("br",null)," 2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1509.06825.pdf"},"Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours, Pinto and Gupta 2016")," ",(0,l.kt)("br",null),"3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/2008.04899.pdf"},"Visual Imitation Made Easy, Young et al. 2020"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/22"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-3 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/22"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-4 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Exploration with Bandits.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/22"),(0,l.kt)("td",{parentName:"tr",align:null},"Imitation Learning"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf"},"Apprenticeship Learning via Inverse Reinforcement Learning, Abbeel and Ng 2004")," ",(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1606.03476.pdf"},"Generative Adversarial Imitation Learning, Ho et al. 2016")," ",(0,l.kt)("br",null),"3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1709.10087.pdf"},"Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations, Rajeswaran et al. 2018"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/29"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-4 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/29"),(0,l.kt)("td",{parentName:"tr",align:null},"Control and planning"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf"},"Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems, Li and Todorov 2004")," ",(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1907.02057.pdf"},"Benchmarking Model-Based Reinforcement Learning, Wang et al. 2019"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"11/5"),(0,l.kt)("td",{parentName:"tr",align:null},"Latent State Discovery -- ",(0,l.kt)("a",{parentName:"td",href:"https://www.microsoft.com/en-us/research/people/jcl/"},"John Langford")),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Guest Lecture")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"11/12"),(0,l.kt)("td",{parentName:"tr",align:null},"Multi-Agent Learning -- ",(0,l.kt)("a",{parentName:"td",href:"https://www.cs.cmu.edu/~noamb/"},"Noam Brown")),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Guest Lecture")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"11/19"),(0,l.kt)("td",{parentName:"tr",align:null},"Unsupervised Reinforcement Learning -- ",(0,l.kt)("a",{parentName:"td",href:"https://cs.nyu.edu/~dy1042/"},"Denis Yarats")),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Guest Lecture")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"12/3"),(0,l.kt)("td",{parentName:"tr",align:null},"Current frontiers"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"12/10"),(0,l.kt)("td",{parentName:"tr",align:null},"Final Project Presentations and Writeups"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})))))}g.isMDXComponent=!0}}]);